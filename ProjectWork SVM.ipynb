{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "np.random.seed(345)\n",
    "\n",
    "bike = pd.read_csv(\"C:/Users/rites/Desktop/5082/hour.csv\")\n",
    "bike_new = bike.drop(['instant', 'dteday', 'casual', 'registered'], axis=1)\n",
    "\n",
    "def split(X, prop = [0.6,0.2,0.2], shuffle=False):\n",
    "    df_index = np.arange(X.shape[0])\n",
    "    if shuffle==True:\n",
    "        np.random.shuffle(df_index)\n",
    "    cut1 = int(np.floor(X.shape[0]*prop[0]))\n",
    "    cut2 = int(np.floor(X.shape[0]*(prop[0]+prop[1])))\n",
    "    train_index = df_index[:cut1]\n",
    "    val_index = df_index[cut1:cut2]\n",
    "    test_index = df_index[cut2:]\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_val = X.iloc[val_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    return [X_train, X_val, X_test]\n",
    "\n",
    "bike_train, bike_val, bike_test = split(bike_new, [0.6,0.2,0.2])\n",
    "\n",
    "bike_train_X = np.array(bike_train.drop(['cnt'], axis=1))\n",
    "bike_train_Y = np.array(bike_train.loc[:,'cnt'])\n",
    "\n",
    "bike_val_X = np.array(bike_val.drop(['cnt'], axis=1))\n",
    "bike_val_Y = np.array(bike_val.loc[:,'cnt'])\n",
    "\n",
    "bike_test_X = np.array(bike_test.drop(['cnt'], axis=1))\n",
    "bike_test_Y = np.array(bike_test.loc[:,'cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RMSE:\n",
    "    def __init__(self, epsilon=0):\n",
    "        self.epsilon = epsilon\n",
    "    def __call__(self, y_truth, y_pred, sample_weights=None):\n",
    "        y_pred = np.round(y_pred)\n",
    "        errors = np.sqrt(np.mean((y_pred-y_truth)**2))\n",
    "        return [y_pred, errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class SVR:\n",
    "    def __init__(self, data, param={}, scorer=None, enable_heuristic=False, enable_kernel_cache=True, max_iteration=3000, verbose=False):\n",
    "        \n",
    "        # Upack training and test data\n",
    "        self.train_X = data['train_X']\n",
    "        self.train_y = data['train_y']\n",
    "        self.test_X = data['test_X']\n",
    "        self.test_y = data['test_y']\n",
    "\n",
    "        # Unpack hyper-parameters\n",
    "        self.C = param.get('C', 0.1)\n",
    "        self.tol = param.get('tol', 1e-2)\n",
    "        self.epsilon = param.get('epsilon', 1e-1)\n",
    "        self.kernel_type = param.get('kernel_type', 'linear')\n",
    "        self.poly_degree = param.get('poly_degree', 3)\n",
    "        self.rbf_sigma = param.get('rbf_sigma', 0.5)\n",
    "\n",
    "        self.scorer = scorer if scorer is not None else RMSE(epsilon=self.epsilon)\n",
    "        self.enable_heuristic = enable_heuristic\n",
    "        self.enable_kernel_cache = enable_kernel_cache\n",
    "        self.max_iteration = max_iteration\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Set kernel function\n",
    "        self.kernels = {\n",
    "            'linear': self._linear_kernel,\n",
    "            'poly': self._poly_kernel,\n",
    "            'rbf': self._rbf_kernel\n",
    "        }\n",
    "        self.kernel = self.kernels[self.kernel_type]\n",
    "\n",
    "        # Precompute kernel cache\n",
    "        if self.enable_kernel_cache:\n",
    "            print('-'*100)\n",
    "            print('[*] Enable kernel cache. Precomputing kernel results for all training examples ...')\n",
    "            self.kernel_cache = self._precompute_kernel_cache()\n",
    "\n",
    "        # Model parameters\n",
    "        self.use_w = True if self.kernel_type == 'linear' else False # If linear kernel is used, use weight to perform prediction instead of alphas.\n",
    "        self.w = np.zeros(self.train_X.shape[1]) # Weight vector: shape(D,) this will be updated when training.\n",
    "        self.b = 0.0 # Bias term: scalar, this will be updated when training.\n",
    "\n",
    "        # SVR needs a pair of lagrange multipliers\n",
    "        # This alpha is not the same as SVC, where each example has one lagrange multiplier `alpha`.\n",
    "        # In SVR, each example has two lagrange multipliers `a1` and `a2`.\n",
    "        # Here, alpha is actually ( a2 - a1 ).\n",
    "        self.alpha = np.zeros(len(self.train_X))\n",
    "\n",
    "        # After training, we can compute biases for support vectors (training examples which 0 < alpha < C)\n",
    "        # for estimating sample mean and sample std of biases.\n",
    "        # For a good learning result, sample std of biases should be small.\n",
    "        self.postcomputed_biases = np.array([None]*len(self.train_X))\n",
    "        self.b_mean = None # Instead of using self.b to make prediction, use self.b_mean when training is done.\n",
    "        self.b_std = None\n",
    "\n",
    "    def train(self, info=''):\n",
    "        \"\"\" Optimize alpha with either simple SMO algorithm or simple SMO combined with Platt's heuristics\n",
    "            In each iteration, the SMO algorithm solves the Lagrangian dual problem\n",
    "            which involves only two Lagrangian multipliers.\n",
    "        \"\"\"\n",
    "        if self.enable_heuristic:\n",
    "            self._heuristic_smo(info)\n",
    "        else:\n",
    "            self._simple_smo(info)\n",
    "\n",
    "    def hypothesis(self, X):\n",
    "        \"\"\" Applying our linear classifier `f(x)` to perform binary classification.\n",
    "            If f(x) >= 0, y(i) = +1\n",
    "            Else    <  0, y(i) = -1\n",
    "            @param `X`: X can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "        \"\"\"\n",
    "        # ---- Not the same as SVC ----\n",
    "        return self._f(X)\n",
    "\n",
    "    def _simple_smo(self, info=''):\n",
    "\n",
    "        num_changed_alphas = 1\n",
    "        iteration = 0\n",
    "\n",
    "        while num_changed_alphas > 0:\n",
    "            num_changed_alphas = 0\n",
    "            for i in range(len(self.train_X)):\n",
    "                if self._violate_KKT_conditions(i):\n",
    "                    j = i\n",
    "                    while(j == i): j = np.random.randint(0, len(self.train_X))\n",
    "                    num_changed_alphas += self._update_alpha_pair(i, j)\n",
    "\n",
    "            if self.verbose and num_changed_alphas == 0:\n",
    "                if info: print('[*] {}'.format(info))\n",
    "                print('[*] Converged at iteration {}.'.format(iteration+1))\n",
    "                print('-'*100)\n",
    "\n",
    "            iteration += 1\n",
    "            if self.verbose and (iteration == 1 or iteration % 100 == 0 or iteration == self.max_iteration):\n",
    "                # Compute training and testing error\n",
    "                train_error = self.scorer(y_truth=self.train_y, y_pred=self.hypothesis(X=self.train_X))\n",
    "                test_error = self.scorer(y_truth=self.test_y, y_pred=self.hypothesis(X=self.test_X))\n",
    "                print('-'*100)\n",
    "                if info: print('[*] {}'.format(info))\n",
    "                print('[*] {} alphas changed.'.format(num_changed_alphas))\n",
    "                print('[*] Iteration: {} | Train error: {} | Test error: {}'.format(iteration, train_error, test_error))\n",
    "\n",
    "            if iteration == self.max_iteration:\n",
    "                print('-'*100)\n",
    "                print('[*] Max iteration acheived.')\n",
    "                break\n",
    "\n",
    "        if self.verbose: print('[*] Averaging post-computed biases as final bias of SVR hypothesis.')\n",
    "        self._postcompute_biases()\n",
    "\n",
    "    def _heuristic_smo(self, info=''):\n",
    "\n",
    "        num_changed_alphas = 0\n",
    "        examine_all = 1\n",
    "        iteration = 0\n",
    "\n",
    "        while num_changed_alphas > 0 or examine_all:\n",
    "            num_changed_alphas = 0\n",
    "            if examine_all:\n",
    "                # Repeated pass iterates over entire examples.\n",
    "                for i in range(len(self.train_X)):\n",
    "                    # alpha_i needs update, select alpha_j (!= alpha_i) to jointly optimize the alpha pair\n",
    "                    if self._violate_KKT_conditions(i):\n",
    "                        j = i\n",
    "                        while(j == i): j = np.random.randint(0, len(self.train_X))\n",
    "                        # Update alpha_i and alpha_j\n",
    "                        num_changed_alphas += self._update_alpha_pair(i, j)\n",
    "\n",
    "                if self.verbose:\n",
    "                    print('-'*100)\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] One passes done.')\n",
    "\n",
    "                if self.verbose and num_changed_alphas == 0:\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] Converged at iteration {}.'.format(iteration+1))\n",
    "                    print('-'*100)\n",
    "                elif self.verbose:\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] Go to repeated passes.')\n",
    "            else:\n",
    "                # Repeated pass iterates over non-boundary examples.\n",
    "                I_non_boundary = np.where(np.logical_and(np.absolute(self.alpha) > 0, np.absolute(self.alpha) < self.C) == True)[0].tolist()\n",
    "                if len(I_non_boundary):\n",
    "                    E_list = np.vectorize(self._E)(I_non_boundary)\n",
    "                    if not max(E_list) - min(E_list) < 1:\n",
    "                        for i in I_non_boundary:\n",
    "                            num_changed_alphas += self._examine_example(i)\n",
    "\n",
    "                if self.verbose and num_changed_alphas == 0:\n",
    "                    print('-'*100)\n",
    "                    if info: print('[*] {}'.format(info))\n",
    "                    print('[*] Repeated passes done. Go back to one pass.')\n",
    "\n",
    "            if examine_all == 1:\n",
    "                # One pass done, go to repeated passes.\n",
    "                examine_all = 0\n",
    "            elif num_changed_alphas == 0:\n",
    "                # Repeated pass done, go back to one pass.\n",
    "                examine_all = 1\n",
    "\n",
    "            iteration += 1\n",
    "            if self.verbose and (iteration == 1 or iteration % 100 == 0 or iteration == self.max_iteration):\n",
    "                # Compute training and testing error\n",
    "                train_error = self.scorer(y_truth=self.train_y, y_pred=self.hypothesis(X=self.train_X))\n",
    "                test_error = self.scorer(y_truth=self.test_y, y_pred=self.hypothesis(X=self.test_X))\n",
    "                print('-'*100)\n",
    "                if info: print('[*] {}'.format(info))\n",
    "                print('[*] {} alphas changed.'.format(num_changed_alphas))\n",
    "                print('[*] Iteration: {} | Train error: {} | Test error: {}'.format(iteration, train_error, test_error))\n",
    "\n",
    "            if iteration == self.max_iteration:\n",
    "                print('-'*100)\n",
    "                print('[*] Max iteration acheived.')\n",
    "                break\n",
    "\n",
    "        if self.verbose: print('[*] Averaging post-computed biases as final bias of SVR hypothesis.')\n",
    "        self._postcompute_biases()\n",
    "\n",
    "    def _violate_KKT_conditions(self, i):\n",
    "        \"\"\" Check if an example violates the KKT conditons \"\"\"\n",
    "\n",
    "        alpha_i = self.alpha[i]\n",
    "        E_i = self._E(i)\n",
    "\n",
    "        # ---- Not the same as SVC ----\n",
    "        if alpha_i == 0 and not (-self.epsilon <= E_i + self.tol and E_i <= self.epsilon + self.tol):\n",
    "            return True\n",
    "        if (-self.C < alpha_i and alpha_i < 0) and not E_i == self.epsilon:\n",
    "            return True\n",
    "        if (0 < alpha_i and alpha_i < self.C) and not E_i == -self.epsilon:\n",
    "            return True\n",
    "        if alpha_i == -self.C and not E_i >= self.epsilon - self.tol:\n",
    "            return True\n",
    "        if alpha_i == self.C and not E_i <= self.epsilon - self.tol:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _examine_example(self, i):\n",
    "        \"\"\" Implement Platt's heuristics to select a good alpha pair to optimize.\n",
    "            (First heuristic is not implemented since it makes training slower)\n",
    "        \"\"\"\n",
    "        # Check if alpha_i needs updating (alpha_i violates KKT conditions)\n",
    "        if self._violate_KKT_conditions(i):\n",
    "\n",
    "            # Retrieve indexes of non boundary examples\n",
    "            I_non_boundary = np.where(np.logical_and(np.absolute(self.alpha) > 0, np.absolute(self.alpha) < self.C) == True)[0].tolist()\n",
    "\n",
    "            # Iterate over non-boundary items, starting at a random position\n",
    "            shuffled_I_non_boundary = np.copy(I_non_boundary)\n",
    "            np.random.shuffle(shuffled_I_non_boundary)\n",
    "            for j in shuffled_I_non_boundary:\n",
    "                if self._update_alpha_pair(i, j):\n",
    "                    return 1\n",
    "\n",
    "            # Iterate over entire items, starting at a random position\n",
    "            I = np.arange(len(self.train_X))\n",
    "            shuffled_I = np.copy(I)\n",
    "            np.random.shuffle(shuffled_I)\n",
    "            for j in shuffled_I:\n",
    "                if self._update_alpha_pair(i, j):\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "    def _update_alpha_pair(self, i, j):\n",
    "        \"\"\" Jointly optimized alpha_i and alpha_j \"\"\"\n",
    "        # Not the alpha pair.\n",
    "        if i == j: return 0\n",
    "\n",
    "        E_i = self._E(i)\n",
    "        E_j = self._E(j)\n",
    "\n",
    "        alpha_i = self.alpha[i]\n",
    "        alpha_j = self.alpha[j]\n",
    "\n",
    "        x_i, x_j, y_i, y_j = self.train_X[i], self.train_X[j], self.train_y[i], self.train_y[j]\n",
    "\n",
    "        # ---- Not the same as SVC ----\n",
    "        L = max(-self.C, alpha_i + alpha_j - self.C)\n",
    "        H = min(self.C, alpha_i + alpha_j + self.C)\n",
    "\n",
    "        # This will not make any progress.\n",
    "        if L == H: return 0\n",
    "\n",
    "        # Compute eta (second derivative of the Lagrange dual function = -eta)\n",
    "        if self.enable_kernel_cache:\n",
    "            eta = self.kernel_cache[i][i] + self.kernel_cache[j][j] - 2*self.kernel_cache[i][j]\n",
    "        else:\n",
    "            eta = self.kernel(x_i, x_i) + self.kernel(x_j, x_j) - 2*self.kernel(x_i, x_j)\n",
    "\n",
    "        # eta > 0 => second derivative(-eta) < 0 => maximum exists.\n",
    "        if eta <= 0: return 0\n",
    "\n",
    "        # ---- Not the same as SVC ----\n",
    "\n",
    "        # Although the update rule of `alpha_j` is a **function of itself**.\n",
    "        # by analysis, we can still update `alpha_j` by trick, since there's only three possible `alpha_j_new`\n",
    "        # See SMO supplement for more details.\n",
    "        delta_E_ij = E_i - E_j\n",
    "\n",
    "        # Calculate list of possible new alphas.\n",
    "        # `x` is a function of `alpha_j_new` and it actually only takes one of {-2, 0, 2}\n",
    "        possible_alpha_j_new = lambda x: alpha_j + (delta_E_ij + x*self.epsilon)/eta\n",
    "        possible_alpha_j_new_pos2 = possible_alpha_j_new(2)\n",
    "        possible_alpha_j_new_zero = possible_alpha_j_new(0)\n",
    "        possible_alpha_j_new_neg2 = possible_alpha_j_new(-2)\n",
    "\n",
    "        # How `alpha_j` is updated depends on various conditions of `r_ij = alpha_i + alpha_j`\n",
    "        r_ij = alpha_i + alpha_j\n",
    "\n",
    "        # Compute new alpha_j and clip it inside [L, H]. This is the update case when eta > 0\n",
    "        if r_ij == 0:\n",
    "            if possible_alpha_j_new_pos2 <= L:\n",
    "                alpha_j_new = L\n",
    "            elif L < possible_alpha_j_new_pos2 and possible_alpha_j_new_pos2 < 0:\n",
    "                alpha_j_new = possible_alpha_j_new_pos2\n",
    "            elif possible_alpha_j_new_neg2 >= H:\n",
    "                alpha_j_new = H\n",
    "            elif 0 < possible_alpha_j_new_neg2 and possible_alpha_j_new_neg2 < H:\n",
    "                alpha_j_new = possible_alpha_j_new_neg2\n",
    "            else:\n",
    "                alpha_j_new = 0\n",
    "\n",
    "        elif 0 < r_ij and r_ij < self.C:\n",
    "            if possible_alpha_j_new_pos2 <= L:\n",
    "                alpha_j_new = L\n",
    "            elif L < possible_alpha_j_new_pos2 and possible_alpha_j_new_pos2 < 0:\n",
    "                alpha_j_new = possible_alpha_j_new_pos2\n",
    "            elif possible_alpha_j_new_zero <= 0:\n",
    "                alpha_j_new = 0\n",
    "            elif 0 < possible_alpha_j_new_zero and possible_alpha_j_new_zero < r_ij:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            elif possible_alpha_j_new_neg2 >= H:\n",
    "                alpha_j_new = H\n",
    "            elif r_ij < possible_alpha_j_new_neg2 and possible_alpha_j_new_neg2 < H:\n",
    "                alpha_j_new = possible_alpha_j_new_neg2\n",
    "            else:\n",
    "                alpha_j_new = r_ij\n",
    "\n",
    "        elif r_ij == self.C:\n",
    "            if possible_alpha_j_new_zero <= L:\n",
    "                alpha_j_new = L\n",
    "            elif L < possible_alpha_j_new_zero and possible_alpha_j_new_zero < H:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            else:\n",
    "                alpha_j_new = H\n",
    "\n",
    "        elif r_ij > self.C:\n",
    "            if possible_alpha_j_new_zero < L:\n",
    "                alpha_j_new = L\n",
    "            elif L <= possible_alpha_j_new_zero and possible_alpha_j_new_zero <= H:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            else:\n",
    "                alpha_j_new = H\n",
    "\n",
    "        elif -self.C < r_ij and r_ij < 0:\n",
    "            if possible_alpha_j_new_pos2 <= L:\n",
    "                alpha_j_new = L\n",
    "            elif L < possible_alpha_j_new_pos2 and possible_alpha_j_new_pos2 < r_ij:\n",
    "                alpha_j_new = possible_alpha_j_new_pos2\n",
    "            elif possible_alpha_j_new_zero <= r_ij:\n",
    "                alpha_j_new = r_ij\n",
    "            elif r_ij < possible_alpha_j_new_zero and possible_alpha_j_new_zero < 0:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            elif possible_alpha_j_new_neg2 >= H:\n",
    "                alpha_j_new = H\n",
    "            elif 0 < possible_alpha_j_new_neg2 and possible_alpha_j_new_neg2 < H:\n",
    "                alpha_j_new = possible_alpha_j_new_neg2\n",
    "            else:\n",
    "                alpha_j_new = 0\n",
    "\n",
    "        elif r_ij == -self.C:\n",
    "            if possible_alpha_j_new_zero <= L:\n",
    "                alpha_j_new = L\n",
    "            elif L < possible_alpha_j_new_zero and possible_alpha_j_new_zero < H:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            else:\n",
    "                alpha_j_new = H\n",
    "\n",
    "        elif r_ij < -self.C:\n",
    "            if possible_alpha_j_new_zero < L:\n",
    "                alpha_j_new = L\n",
    "            elif L <= possible_alpha_j_new_zero and possible_alpha_j_new_zero <= H:\n",
    "                alpha_j_new = possible_alpha_j_new_zero\n",
    "            else:\n",
    "                alpha_j_new = H\n",
    "\n",
    "        # Compute new alpha_i based on new alpha_j\n",
    "        alpha_i_new = alpha_i - (alpha_j_new - alpha_j)\n",
    "\n",
    "        # Compute step sizes\n",
    "        delta_alpha_i = alpha_i_new - alpha_i\n",
    "        delta_alpha_j = alpha_j_new - alpha_j\n",
    "\n",
    "        # Update weight vector\n",
    "        if self.use_w:\n",
    "            self.w = self.w + delta_alpha_i*x_i + delta_alpha_j*x_j\n",
    "\n",
    "        # Update b\n",
    "        if self.enable_kernel_cache:\n",
    "            b_i = self.b - E_i - delta_alpha_i*self.kernel_cache[i][i] - delta_alpha_j*self.kernel_cache[i][j]\n",
    "            b_j = self.b - E_j - delta_alpha_i*self.kernel_cache[i][j] - delta_alpha_j*self.kernel_cache[j][j]\n",
    "        else:\n",
    "            b_i = self.b - E_i - delta_alpha_i*self.kernel(x_i, x_i) - delta_alpha_j*self.kernel(x_i, x_j)\n",
    "            b_j = self.b - E_j - delta_alpha_i*self.kernel(x_i, x_j) - delta_alpha_j*self.kernel(x_j, x_j)\n",
    "        self.b = (b_i + b_j)/2\n",
    "        if (alpha_i_new > 0 and alpha_i_new < self.C):\n",
    "            self.b = b_i\n",
    "        if (alpha_j_new > 0 and alpha_j_new < self.C):\n",
    "            self.b = b_j\n",
    "\n",
    "        # Update the alpha pair\n",
    "        self.alpha[i] = alpha_i_new\n",
    "        self.alpha[j] = alpha_j_new\n",
    "\n",
    "        return 1\n",
    "\n",
    "    def _f(self, X):\n",
    "        \"\"\" Linear classifier `f(x) = wx + b`, used when training or making predictions.\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "        \"\"\"\n",
    "        # Use b_mean to make predictions when training is done.\n",
    "        b = self.b_mean if self.b_mean is not None else self.b\n",
    "\n",
    "        if self.use_w:\n",
    "            # Speed up by using computed weight only when linear kernel is used.\n",
    "            return np.dot(X, self.w) + b\n",
    "        else:\n",
    "            # If X is single example\n",
    "            if X.ndim == 1:\n",
    "                # ---- Not the same as SVC ----\n",
    "                return np.dot(self.alpha, self.kernel(self.train_X, X)) + b\n",
    "            # Multiple examples\n",
    "            elif X.ndim == 2:\n",
    "                return np.array([np.dot(self.alpha, self.kernel(self.train_X, _X)) + b for _X in X])\n",
    "\n",
    "    def _E(self, i):\n",
    "        \"\"\" Prediction error: _f(x_i) - y_i, used when training. \"\"\"\n",
    "        if self.enable_kernel_cache:\n",
    "            # ---- Not the same as SVC ----\n",
    "            return np.dot(self.alpha, self.kernel_cache[i]) + self.b - self.train_y[i]\n",
    "        else:\n",
    "            return self._f(self.train_X[i]) - self.train_y[i]\n",
    "\n",
    "    def _precompute_kernel_cache(self):\n",
    "        \"\"\" If self.enable_kernel_cache is True, then precompute kernel results for all training examples.\n",
    "            This can speed up training but need time to initialize when data is large.\n",
    "        \"\"\"\n",
    "        kernel_cache = np.zeros((len(self.train_X), len(self.train_X)))\n",
    "        for i, x_i in enumerate(self.train_X):\n",
    "            for j, x_j in enumerate(self.train_X):\n",
    "                kernel_cache[i][j] = self.kernel(x_i, x_j)\n",
    "        return kernel_cache\n",
    "\n",
    "    def _postcompute_biases(self):\n",
    "        \"\"\" Post-computed biases for non-boundary training examples (support vectors) when training is done.\n",
    "            This is for estimat ing sample mean and sample std of biases.\n",
    "            For a good learning result, sample std of biases should be small.\n",
    "        \"\"\"\n",
    "        # ---- Not the same as SVC ----\n",
    "        def _b(i):\n",
    "            if self.enable_kernel_cache:\n",
    "                return self.train_y[i] - np.dot(self.alpha, self.kernel_cache[i])\n",
    "            else:\n",
    "                return self.train_y[i] - self._f(self.train_X[i])\n",
    "\n",
    "        I_non_boundary = np.where(np.logical_and(np.absolute(self.alpha) > 0, np.absolute(self.alpha) < self.C) == True)[0].tolist()\n",
    "\n",
    "        if len(I_non_boundary):\n",
    "            biases = np.vectorize(_b)(I_non_boundary)\n",
    "            self.b_mean = np.mean(biases)\n",
    "            self.b_std = np.sqrt(np.sum((biases - self.b_mean)**2) / (len(biases) - 1))\n",
    "            self.postcomputed_biases[I_non_boundary] = biases\n",
    "\n",
    "    def _linear_kernel(self, X, x):\n",
    "        \"\"\" Linear kernel:\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "            @param `x`: `x` can only be a single example with shape(D,)\n",
    "        \"\"\"\n",
    "        return np.dot(X, x)\n",
    "\n",
    "    def _poly_kernel(self, X, x):\n",
    "        \"\"\" Polynomial kernel:\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "            @param `x`: `x` can only be a single example with shape(D,)\n",
    "        \"\"\"\n",
    "        return (1 + np.dot(X, x))**self.poly_degree\n",
    "\n",
    "    def _rbf_kernel(self, X, x):\n",
    "        \"\"\" RBF (guassian) kernel:\n",
    "            @param `X`: `X` can be a single example with shape(D,) or multiple examples with shape(N, D)\n",
    "            @param `x`: `x` can only be a single example with shape(D,)\n",
    "        \"\"\"\n",
    "        # If X is single example\n",
    "        if X.ndim == 1:\n",
    "            sqrt_norm = np.linalg.norm(X - x)**2\n",
    "        # Multiple examples\n",
    "        elif X.ndim == 2:\n",
    "            sqrt_norm = np.linalg.norm(X - x, axis=1)**2\n",
    "\n",
    "        return np.exp(-sqrt_norm / (2.0 * (self.rbf_sigma**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "data = {'train_X': bike_train_X,\n",
    "        'train_y': bike_train_Y,\n",
    "        'test_X': bike_val_X,\n",
    "        'test_y': bike_val_Y }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  113.211678492\n",
      "Validation Error:  202.749468435\n",
      "---------------------------\n",
      "C: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  117.847303156\n",
      "Validation Error:  211.239478735\n",
      "---------------------------\n",
      "C: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  112.878937162\n",
      "Validation Error:  205.93055867\n",
      "---------------------------\n",
      "C: 0.9\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  122.391641136\n",
      "Validation Error:  220.29825282\n",
      "---------------------------\n",
      "C: 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  114.459752775\n",
      "Validation Error:  210.848217104\n",
      "---------------------------\n",
      "C: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  122.999925537\n",
      "Validation Error:  239.886929756\n",
      "---------------------------\n",
      "C: 0.001\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  129.873832366\n",
      "Validation Error:  256.242833126\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(252)\n",
    "for C in [5,2,1,0.9,0.1,0.01,0.001]:\n",
    "    print(\"C:\",C)\n",
    "    param = {'C':C, 'epsilon':0.1, 'tol': 1e-2, 'kernel_type': 'linear'}\n",
    "    model = SVR(data= data, param= param, enable_kernel_cache=False, max_iteration=100, verbose=False)\n",
    "    model.train(info='Train on specified parameter: {}'.format(param))\n",
    "    mean_absolute_epsilon_error = RMSE(epsilon=0.1)\n",
    "    train_pred, train_error = mean_absolute_epsilon_error(y_truth=data['train_y'], y_pred=model.hypothesis(X=data['train_X']))\n",
    "    val_pred, val_error = mean_absolute_epsilon_error(y_truth=data['test_y'], y_pred=model.hypothesis(X=data['test_X']))\n",
    "    test_pred, test_error = mean_absolute_epsilon_error(y_truth=bike_test_Y, y_pred=model.hypothesis(X=bike_test_X))\n",
    "    print(\"Train Error: \", train_error)\n",
    "    print(\"Validation Error: \", val_error)\n",
    "    #print(\"Test Error: \", test_error)\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  112.877307143\n",
      "Validation Error:  203.700089371\n",
      "---------------------------\n",
      "epsilon: 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  117.035632657\n",
      "Validation Error:  212.502828114\n",
      "---------------------------\n",
      "epsilon: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  110.155752136\n",
      "Validation Error:  197.558501772\n",
      "---------------------------\n",
      "epsilon: 0.03\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  112.302825089\n",
      "Validation Error:  204.262618224\n",
      "---------------------------\n",
      "epsilon: 0.001\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  121.817667701\n",
      "Validation Error:  217.263574661\n",
      "---------------------------\n",
      "epsilon: 0.003\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  110.817505894\n",
      "Validation Error:  201.145373921\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(252)\n",
    "for eps in [1,0.1,0.01,0.03,0.001, 0.003]:\n",
    "    print(\"epsilon:\",eps)\n",
    "    param = {'C':1, 'epsilon':eps, 'tol': 1e-2, 'kernel_type': 'linear'}\n",
    "    model = SVR(data= data, param= param, enable_kernel_cache=False, max_iteration=100, verbose=False)\n",
    "    model.train(info='Train on specified parameter: {}'.format(param))\n",
    "    mean_absolute_epsilon_error = RMSE(epsilon=0.1)\n",
    "    train_pred, train_error = mean_absolute_epsilon_error(y_truth=data['train_y'], y_pred=model.hypothesis(X=data['train_X']))\n",
    "    val_pred, val_error = mean_absolute_epsilon_error(y_truth=data['test_y'], y_pred=model.hypothesis(X=data['test_X']))\n",
    "    test_pred, test_error = mean_absolute_epsilon_error(y_truth=bike_test_Y, y_pred=model.hypothesis(X=bike_test_X))\n",
    "    print(\"Train Error: \", train_error)\n",
    "    print(\"Validation Error: \", val_error)\n",
    "    #print(\"Test Error: \", test_error)\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tolerance: 0.01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  111.219024155\n",
      "Validation Error:  202.238719617\n",
      "---------------------------\n",
      "tolerance: 0.001\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  113.742201007\n",
      "Validation Error:  208.09926498\n",
      "---------------------------\n",
      "tolerance: 0.0001\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  116.646526734\n",
      "Validation Error:  211.005779566\n",
      "---------------------------\n",
      "tolerance: 1e-05\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  114.955531993\n",
      "Validation Error:  208.424494363\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(252)\n",
    "for tol in [0.01,0.001,0.0001,0.00001]:\n",
    "    print(\"tolerance:\",tol)\n",
    "    param = {'C':1, 'epsilon':0.003, 'tol': tol, 'kernel_type': 'linear'}\n",
    "    model = SVR(data= data, param= param, enable_kernel_cache=False, max_iteration=100, verbose=False)\n",
    "    model.train(info='Train on specified parameter: {}'.format(param))\n",
    "    mean_absolute_epsilon_error = RMSE(epsilon=0.1)\n",
    "    train_pred, train_error = mean_absolute_epsilon_error(y_truth=data['train_y'], y_pred=model.hypothesis(X=data['train_X']))\n",
    "    val_pred, val_error = mean_absolute_epsilon_error(y_truth=data['test_y'], y_pred=model.hypothesis(X=data['test_X']))\n",
    "    test_pred, test_error = mean_absolute_epsilon_error(y_truth=bike_test_Y, y_pred=model.hypothesis(X=bike_test_X))\n",
    "    print(\"Train Error: \", train_error)\n",
    "    print(\"Validation Error: \", val_error)\n",
    "    #print(\"Test Error: \", test_error)\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxi: 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  113.563842536\n",
      "Validation Error:  206.002054997\n",
      "---------------------------\n",
      "maxi: 500\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  112.293106325\n",
      "Validation Error:  204.035513561\n",
      "---------------------------\n",
      "maxi: 1000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  112.965081416\n",
      "Validation Error:  205.317886934\n",
      "---------------------------\n",
      "maxi: 3000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  112.049328763\n",
      "Validation Error:  203.688203051\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(252)\n",
    "for maxi in [100,500,1000,3000]:\n",
    "    print(\"maxi:\",maxi)\n",
    "    param = {'C':1, 'epsilon':0.003, 'tol': 0.0001, 'kernel_type': 'linear'}\n",
    "    model = SVR(data= data, param= param, enable_kernel_cache=False, max_iteration=maxi, verbose=False)\n",
    "    model.train(info='Train on specified parameter: {}'.format(param))\n",
    "    mean_absolute_epsilon_error = RMSE(epsilon=0.1)\n",
    "    train_pred, train_error = mean_absolute_epsilon_error(y_truth=data['train_y'], y_pred=model.hypothesis(X=data['train_X']))\n",
    "    val_pred, val_error = mean_absolute_epsilon_error(y_truth=data['test_y'], y_pred=model.hypothesis(X=data['test_X']))\n",
    "    test_pred, test_error = mean_absolute_epsilon_error(y_truth=bike_test_Y, y_pred=model.hypothesis(X=bike_test_X))\n",
    "    print(\"Train Error: \", train_error)\n",
    "    print(\"Validation Error: \", val_error)\n",
    "    #print(\"Test Error: \", test_error)\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  110.790264415\n",
      "Validation Error:  200.946685674\n",
      "Test Error:  203.109570672\n"
     ]
    }
   ],
   "source": [
    "# Optimized hyperparameters\n",
    "\n",
    "np.random.seed(250)\n",
    "param = {'C':1, 'epsilon':0.0001, 'tol': 0.00001, 'kernel_type': 'linear'}\n",
    "model = SVR(data= data, param= param, enable_kernel_cache=False, max_iteration=500, enable_heuristic=True)\n",
    "model.train(info='Train on specified parameter: {}'.format(param))\n",
    "mean_absolute_epsilon_error = RMSE(epsilon=0.0001)\n",
    "train_pred, train_error = mean_absolute_epsilon_error(y_truth=data['train_y'], y_pred=model.hypothesis(X=data['train_X']))\n",
    "val_pred, val_error = mean_absolute_epsilon_error(y_truth=data['test_y'], y_pred=model.hypothesis(X=data['test_X']))\n",
    "test_pred, test_error = mean_absolute_epsilon_error(y_truth=bike_test_Y, y_pred=model.hypothesis(X=bike_test_X))\n",
    "print(\"Train Error: \", train_error)\n",
    "print(\"Validation Error: \", val_error)\n",
    "print(\"Test Error: \", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  207.736751853\n",
      "Validation Error:  353.043575778\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Rbf Kernel\n",
    "param = {'C':1, 'epsilon':0.0001, 'tol': 0.00001, 'kernel_type': 'rbf', 'sigma': 0.5}\n",
    "model = SVR(data= data, param= param, enable_kernel_cache=False, enable_heuristic=True, max_iteration=500, verbose=False)\n",
    "model.train(info='Train on specified parameter: {}'.format(param))\n",
    "mean_absolute_epsilon_error = RMSE(epsilon=0.1)\n",
    "train_pred, train_error = mean_absolute_epsilon_error(y_truth=data['train_y'], y_pred=model.hypothesis(X=data['train_X']))\n",
    "val_pred, val_error = mean_absolute_epsilon_error(y_truth=data['test_y'], y_pred=model.hypothesis(X=data['test_X']))\n",
    "test_pred, test_error = mean_absolute_epsilon_error(y_truth=bike_test_Y, y_pred=model.hypothesis(X=bike_test_X))\n",
    "print(\"Train Error: \", train_error)\n",
    "print(\"Validation Error: \", val_error)\n",
    "#print(\"Test Error: \", test_error)\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[*] Max iteration acheived.\n",
      "Train Error:  679.999410185\n",
      "Validation Error:  621.56007253\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Polynomial kernel\n",
    "param = {'C':1, 'epsilon':0.003, 'tol': 0.0001, 'kernel_type': 'poly', 'poly_degree': 3}\n",
    "model = SVR(data= data, param= param, enable_kernel_cache=False, max_iteration=100, verbose=False)\n",
    "model.train(info='Train on specified parameter: {}'.format(param))\n",
    "mean_absolute_epsilon_error = RMSE(epsilon=0.1)\n",
    "train_pred, train_error = mean_absolute_epsilon_error(y_truth=data['train_y'], y_pred=model.hypothesis(X=data['train_X']))\n",
    "val_pred, val_error = mean_absolute_epsilon_error(y_truth=data['test_y'], y_pred=model.hypothesis(X=data['test_X']))\n",
    "test_pred, test_error = mean_absolute_epsilon_error(y_truth=bike_test_Y, y_pred=model.hypothesis(X=bike_test_X))\n",
    "print(\"Train Error: \", train_error)\n",
    "print(\"Validation Error: \", val_error)\n",
    "#print(\"Test Error: \", test_error)\n",
    "print(\"---------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
